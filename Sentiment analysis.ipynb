{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we'll be the Multi-Domain Sentiment Dataset that is availible in the following location :\n",
    "https://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html. \n",
    "\n",
    "The Multi-Domain Sentiment Dataset contains product reviews taken from Amazon.com from 4 product types (domains): Kitchen, Books, DVDs, and Electronics. Each domain has several thousand reviews, but the exact number varies by domain. Reviews contain star ratings (1 to 5 stars) that can be converted into binary labels if needed.\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start as usual by importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T11:20:01.269017Z",
     "start_time": "2018-04-07T11:20:01.255673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/riaanmostert/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/riaanmostert/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from six.moves import urllib\n",
    "import os\n",
    "import tarfile\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download(['punkt','wordnet'])\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll download the required file and extract it to our working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:50:03.095310Z",
     "start_time": "2018-04-07T10:50:03.065836Z"
    }
   },
   "outputs": [],
   "source": [
    "download_root = \"https://www.cs.jhu.edu/~mdredze/datasets/sentiment/\"\n",
    "file_name = \"domain_sentiment_data.tar.gz\"\n",
    "sentiment_url =  download_root + file_name\n",
    "\n",
    "\n",
    "def download_extract(url,location):\n",
    "    '''\n",
    "    url: ulr location where the data resides\n",
    "    location: location on workstation the data needs to be copied to\n",
    "    '''\n",
    "    gz_path =  os.path.join(location,file_name)\n",
    "    _ = urllib.request.urlretrieve(url = sentiment_url,filename=gz_path)\n",
    "    gz_folder = tarfile.open(gz_path)\n",
    "    gz_folder.extractall(path=location)\n",
    "    gz_folder.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:22.130841Z",
     "start_time": "2018-04-07T10:50:03.098631Z"
    }
   },
   "outputs": [],
   "source": [
    "download_extract(url= sentiment_url,location=os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll import a list of stopwords that were downloaded from : http://www.lextek.com/manuals/onix/stopwords1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:22.280131Z",
     "start_time": "2018-04-07T10:51:22.134124Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = set(w.rstrip() for w in open('stopwords.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T15:42:09.033172Z",
     "start_time": "2018-04-05T15:42:09.021039Z"
    }
   },
   "source": [
    "By looking at the list of stopwords, we see some interesting inclusions, like 'great','important', 'problem'. These words will be excluded since they may be too restrictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:22.289543Z",
     "start_time": "2018-04-07T10:51:22.283824Z"
    }
   },
   "outputs": [],
   "source": [
    "not_stopwords= ['best','better','good','great',\n",
    "'greater','greatest','important','interesting','problem','problems','work',\n",
    "'worked','working','works','would']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:22.297659Z",
     "start_time": "2018-04-07T10:51:22.292881Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = [x for x in stopwords if x not in not_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T12:47:28.890250Z",
     "start_time": "2018-04-02T12:47:28.885622Z"
    }
   },
   "source": [
    "Since the reviews are in a XML format, we'll make use of the BeautifulSoup library to assit us in importing this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:23.069182Z",
     "start_time": "2018-04-07T10:51:22.300870Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_reviews = (BeautifulSoup(open('sorted_data_acl/electronics/positive.review')\n",
    "                                  .read(),'lxml'))\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:23.915722Z",
     "start_time": "2018-04-07T10:51:23.071436Z"
    }
   },
   "outputs": [],
   "source": [
    "negative_reviews = (BeautifulSoup(open('sorted_data_acl/electronics/negative.review')\n",
    "                                  .read(),'lxml'))\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:23.925120Z",
     "start_time": "2018-04-07T10:51:23.918611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 1000\n",
      "Number of negative reviews: 1000\n"
     ]
    }
   ],
   "source": [
    "print('Number of positive reviews: {}'.format(len(positive_reviews)))\n",
    "print('Number of negative reviews: {}'.format(len(negative_reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the number of positive and negative reviews are evenly split, which is great for our modelling exercise. Next we'll write a function that will assist us in tokenizing the reviews by first converting the text to lowercase, only keeping words whose length is greater than 2, Lemmatization to return the base or dictionary form of a word,and lastly remove the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:23.944889Z",
     "start_time": "2018-04-07T10:51:23.928813Z"
    }
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    tokens = [t for t in tokens if len(t) > 2 ]\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in  stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a word-to-index map so that we can create our word-frequency vectors later. Let's also save the tokenized versions so we don't have to tokenize again later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:32.031935Z",
     "start_time": "2018-04-07T10:51:23.948617Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index_map = {}\n",
    "current_index = 0\n",
    "\n",
    "positive_tokenize = []\n",
    "nagative_tokenize = []\n",
    "\n",
    "\n",
    "for review in positive_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenize.append(tokens)\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "            \n",
    "            \n",
    "for review in negative_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    nagative_tokenize.append(tokens)\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:32.050146Z",
     "start_time": "2018-04-07T10:51:32.034338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['purchased',\n",
       " 'unit',\n",
       " 'due',\n",
       " 'frequent',\n",
       " 'blackout',\n",
       " 'power',\n",
       " 'supply',\n",
       " 'bad',\n",
       " 'run',\n",
       " 'cable',\n",
       " 'modem',\n",
       " 'router',\n",
       " 'lcd',\n",
       " 'monitor',\n",
       " 'minute',\n",
       " 'time',\n",
       " 'save',\n",
       " 'work',\n",
       " 'shut',\n",
       " 'equally',\n",
       " 'important',\n",
       " 'electronics',\n",
       " 'receiving',\n",
       " 'clean',\n",
       " 'power',\n",
       " 'feel',\n",
       " 'investment',\n",
       " 'minor',\n",
       " 'compared',\n",
       " 'loss',\n",
       " 'valuable',\n",
       " 'data',\n",
       " 'failure',\n",
       " 'equipment',\n",
       " 'due',\n",
       " 'power',\n",
       " 'spike',\n",
       " 'irregular',\n",
       " 'power',\n",
       " 'supply',\n",
       " 'amazon',\n",
       " 'business',\n",
       " 'day']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tokenize[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll write a function to assist us to first calculate the word frequency per review and then calculate the proportion of times that word appear in a particular review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:32.069765Z",
     "start_time": "2018-04-07T10:51:32.052854Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokens_to_vector(tokens,label):\n",
    "    x = np.zeros(len(word_index_map)+1)\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x/ x.sum()\n",
    "    x[-1] = label\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:32.398200Z",
     "start_time": "2018-04-07T10:51:32.074341Z"
    }
   },
   "outputs": [],
   "source": [
    "N = len(positive_reviews) + len(negative_reviews)\n",
    "data = np.zeros((N,len(word_index_map)+1))\n",
    "i = 0\n",
    "\n",
    "for token in positive_tokenize:\n",
    "    xy = tokens_to_vector(token,1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "    \n",
    "for token in nagative_tokenize:\n",
    "    xy = tokens_to_vector(token,0)\n",
    "    data[i,:] = xy\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll shuffle the data and split it into a training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:32.499945Z",
     "start_time": "2018-04-07T10:51:32.400496Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(567)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:32.507838Z",
     "start_time": "2018-04-07T10:51:32.502196Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X[:-100,]\n",
    "y_train = y[:-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:32.515189Z",
     "start_time": "2018-04-07T10:51:32.510930Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = X[-100:,]\n",
    "y_test = y[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:33.529109Z",
     "start_time": "2018-04-07T10:51:32.518274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:33.546803Z",
     "start_time": "2018-04-07T10:51:33.531898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy rate 0.75 \n"
     ]
    }
   ],
   "source": [
    "print('Accuracy rate {:1.2f} '.format(model.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for the first try! Let's look at the weights for the different words to see whether it makes sense. We'll use a threshold of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:51:33.600940Z",
     "start_time": "2018-04-07T10:51:33.550061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit -0.5192522345018026\n",
      "bad -0.7216540338202034\n",
      "cable 0.5665912597138244\n",
      "time -0.785508495761577\n",
      "'ve 0.7423653151173176\n",
      "month -0.8401167522996076\n",
      "problem 0.6864006668703992\n",
      "good 2.0418603056513405\n",
      "sound 0.9271781879037885\n",
      "lot 0.6969369705471137\n",
      "n't -2.214416060680395\n",
      "easy 1.6964124737926294\n",
      "quality 1.2268962576370404\n",
      "company -0.5812177116732773\n",
      "card -0.5175925736432473\n",
      "best 1.1583655523035772\n",
      "item -1.0006937155653035\n",
      "working -0.6278319414574575\n",
      "wa -1.4985996144368325\n",
      "perfect 1.0105649222514923\n",
      "fast 0.8750718518437195\n",
      "ha 0.5226216515686919\n",
      "price 2.532786135135857\n",
      "great 3.8206824310571283\n",
      "money -1.125056780650876\n",
      "memory 0.9158174736872906\n",
      "would -0.9681810064705028\n",
      "buy -1.0918928377260069\n",
      "worked -0.8976187684010911\n",
      "happy 0.6211608325395602\n",
      "pretty 0.6295695652542614\n",
      "doe -1.1507679196744474\n",
      "highly 1.0180175252415575\n",
      "recommend 0.6472559119760061\n",
      "customer -0.6012834046036264\n",
      "support -0.81876351877893\n",
      "little 0.7997269220004799\n",
      "returned -0.7761606505264842\n",
      "excellent 1.2630159048311336\n",
      "love 1.0839995784142908\n",
      "useless -0.5198526770270236\n",
      "week -0.6950814508028877\n",
      "using 0.6767461997384369\n",
      "poor -0.7546683609706677\n",
      "tried -0.8406230200590621\n",
      "radio -0.5354329819541546\n",
      "try -0.6174624768587762\n",
      "space 0.5978450902425985\n",
      "comfortable 0.6245717108042934\n",
      "photo 0.5153974257857556\n",
      "hour -0.5419686932846809\n",
      "expected 0.5590063684045372\n",
      "speaker 0.7984277472052069\n",
      "warranty -0.6231783730845203\n",
      "stopped -0.5226684097905256\n",
      "junk -0.5578537011096577\n",
      "returning -0.545630335387805\n",
      "paper 0.610027659719277\n",
      "return -1.2124434237471007\n",
      "waste -0.9998803946353099\n",
      "refund -0.6224185457408893\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "for word, index in word_index_map.items():\n",
    "    weight = model.coef_[0][index]\n",
    "    \n",
    "    if abs(weight) > threshold:\n",
    "        print(word,weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights mostly make sense. For example, we see that reviews containing the word 'junk' is more likely to be a negative rewiew. Likewise reviews containing the word 'great' has a large positive weight and increase the likelihood of the review being positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fit a few additional models to see whether we can imporve the accuracy rate: A naive bayes, a random forest and an AdaBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T11:20:24.151771Z",
     "start_time": "2018-04-07T11:20:24.147414Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T11:20:56.070563Z",
     "start_time": "2018-04-07T11:20:56.017792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy rate 0.76 \n"
     ]
    }
   ],
   "source": [
    "print('Accuracy rate {:1.2f} '.format(nb_model.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:55:42.827364Z",
     "start_time": "2018-04-07T10:55:42.823495Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=200,random_state=893)\n",
    "rf_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T10:56:11.727943Z",
     "start_time": "2018-04-07T10:56:11.685927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy rate 0.79 \n"
     ]
    }
   ],
   "source": [
    "print('Accuracy rate {:1.2f} '.format(rf_model.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T11:13:08.882269Z",
     "start_time": "2018-04-07T11:00:19.943642Z"
    }
   },
   "outputs": [],
   "source": [
    "ada_model =  AdaBoostClassifier(n_estimators=200,random_state=893)\n",
    "ada_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T11:18:09.195379Z",
     "start_time": "2018-04-07T11:18:09.032906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy rate 0.80 \n"
     ]
    }
   ],
   "source": [
    "print('Accuracy rate {:1.2f} '.format(ada_model.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AdaBoost model is giving the best performance - and this is just using the default hyperparameters. We can achieve superior results by making use of a Recursive neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
